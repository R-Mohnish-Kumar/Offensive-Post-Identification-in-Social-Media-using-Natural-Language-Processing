# Offensive-Post-Identification-in-Social-Media-using-Natural-Language-Processing

This project aims to identify offensive posts in social media using Natural Language Processing (NLP) techniques. The dataset consists of tweets, which undergo preprocessing, feature extraction, and classification using various machine learning models. The best-performing model is fine-tuned for optimal performance.

---

## Features
1. Data preprocessing: Tokenization, Lemmatization, Slang word replacement, Unicode normalization, punctuation & emoji removal.
2. Feature extraction using vectorization techniques.
3. Machine learning classification using multiple models.
4. Performance evaluation and model selection.
5. Hyperparameter tuning for optimal results.
   
---

## Data Preprocessing
1. **Tokenization:** Breaking text into words.
2. **Lemmatization:**
   - Normal Lemmatization
   - Spacy Lemmatization
   - WordNet Lemmatization with POS tagging
   - Combined Spacy & Normal Lemmatization
3. **Slang Word Replacement:** Using a dictionary to replace informal words.
4. **Unicode Normalization:** Standardizing text formats.
5. **Punctuation and Emoji Removal:** Cleaning unnecessary characters.
6. **Vectorization:** Transforming text into numerical features.

---

## Algorithms
1. Logistic Regression
2. Support Vector Machine (SVM)
3. XGBoost (Extreme Gradient Boosting)
4. Random Forest Classifier
5. Gaussian Naive Bayes
6. Traditinal Ensemble Methodology

---

## Installation and Usage

1. Clone the repository:
   ```bash
   git clone https://github.com/R-Mohnish-Kumar/Offensive-Post-Identification-in-Social-Media-using-Natural-Language-Processing.git
   ```
2. Open the Jupyter Notebook file:
   ```bash
   jupyter notebook Offensive_Post_Identification_in_Social_Media_(NLP) (1).ipynb
   ```
3. Execute the cells step-by-step to analyze the data and train the models.

